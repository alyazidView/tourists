---
title: "Tourist - Airbnb Canada Quebec"
format: 
    html:
        code-fold: true
jupyter: python3
author: 
- Rehab Alaswad
- Rawan Aljohani
- Alyazid Alhumaydani
- Ibrahim Alghrabi
---



# Introduction:
In this report, we will study the Airbnb Canada dataset and try to make recommendation of a listing to an investor based on how close the listing is to nature. 

## About the Dataset: 
The dataset was obtained from Inside Airbnb, and it provides data and advocacy about Airbnb's impact on resdential communities. Inside Airbnb provides data for many countries but we chose Canda becuase of great nature in Canada.

## Our Scenario 
To study the Airbnb hospitality market for an investor interested in purchasing or building a property in one of the tourist twons near great nature. 

## Data Dictionary:
___________
| *Variable* | *Description*|
|:---------|:---------:|
|id | Airbnb's unique identifier for the listing.|
|name | Name of the listing.|
|host_id|Airbnb's unique identifier for the host.|
|host_name| Name of the host.|
|neighbourhood| Name of the neighbourhood. |
|latitude | Uses the World Geodetic System (WGS84)|
|longitude | Uses the World Geodetic System (WGS84)|
|room_type | Type of the listing; entire house, private room, or shared room.|
|price| Price of the listing in Canadian dollars.|
|minimum_nights| Number of minimum stays in the listing.|
|last_review|The date of the last review.|
|review_per_month| Number of review the listing has over the lifetime of the listing|
|availability_365| The availability of the listing in the next 365 days.|
|number_of_review_itm|Number of review the listing has in the past 12 month.|
|license| The license/permit/registration number.|

# Data Cleaning:

First importing the data and show small sample of the dataset.
```{python}
# Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels as sm 
import plotly.express as px
import plotly.io as pio
from matplotlib import colors
import folium
import folium.plugins
import branca.colormap as cmp
from math import radians, cos, sin, asin, sqrt
sns.set_style('darkgrid')
pio.templates.default = 'seaborn'
df = pd.read_csv('../data/canada/listings.csv')
df.sample(4)
```

\ 

```{python}
df.info()
```

This datasets has a total of 13621 observation (rows), and 18 variable (columns). We note that the neighbourhood_group has it all entries as NaN, license column also has many missing values. Since both of these columns are not of interest we will remove them.  

```{python}
df.drop('neighbourhood_group', axis =1, inplace=True)
df.drop('license',axis =1, inplace=True)
```

Lets have a look at the data type of each columns.
```{python}
df.dtypes
```

Since IDs usually are not used in mathematical operation, it would be wise to change them from integer variable into strings. 

```{python}
df['id'] = df['id'].astype(str)
df['host_id'] = df['host_id'].astype(str)
print(f"id column type: {df.id.dtypes}")
print(f"id_host column type: {df.host_id.dtypes}")
```

Lets look at descriptive statistic of the numerical columns: 

```{python}
df.describe()
```

\ 

From the above table, we note the following:

- The maximum number of minimum nights 11684, where the mean is around 15 nights.
- The maximum price is 105634 \$ with mean of 169 \$ and standard variation of 958 \$.

Lets investigate these extream points 

```{python}
df[(df['minimum_nights'] > 11000) | (df['price'] > 100000)]
print(f"The number of observation with price greater than 4000$ is: {df[df['price'] >= 4000].count()['id']}")
df = df[df['minimum_nights']< 11000]
df = df[df['price'] < 4000]
```

Both of these listing are private rooms, it seems those entries are an error so we will remove them. Also in this study I will only include listing of price less than 4000 $, since they only represent 15 observation out of 13600.

Distinct neighbourhood in the datasets.
```{python}
print(f"Number of distinct neighbourhood: {df['neighbourhood'].nunique()}")
df.neighbourhood.unique()
```

# Supporting Dataset:
This dataset presents the place of interest in the City of Montreal. The only places of interest to us are parks, so we can make our recommendation based on listing close to nature. We will drop any observation of missing values. 
```{python}
df3 = pd.read_csv('../data/canada/lieux_d_interet.csv') #load the data
df3.dropna(how='any', inplace=True) #drop any missing value
df3 = df3[df3['Type'].isin(["Parc", "Jardin communautaire"])] #pick only places of interest related parks
df3 #only 4 parks
```



There is only four parks in this dataset. We will use the latitude, and longitude columns of the new dataset, to make two new variable in the original dataset. The new columns are:
'distance_to_nearest_park', 'nearest_park'

- nearest_park: the nearest park.
- distance_to_nearest_park: distance of the closet park in km.

The equation used will calculate the orthodromic distance (i.e. the shortest distance between two points on earth's surface), meaning it will not take into acoount the road distance or traffic. 

```{python}
obs  = list(zip(df3['Latitude'], df3['Longitude']))
def cal_dist1(temp):
    lat1 = temp['latitude'] 
    lon1 = temp['longitude']
    lat1 = radians(lat1)
    lon1 = radians(lon1)
    results= []
    for i in range(len(obs)):
        lon2 = radians(obs[i][1])
        lat2 = radians(obs[i][0])
        # Haversine formula
        dlon = lon2 - lon1
        dlat = lat2 - lat1
        a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
        c = 2 * asin(sqrt(a))
        # Radius of earth in kilometers. Use 3956 for miles
        r = 6371
        # calculate the result
        results.append(c * r * 1.609344)
    
    min_val = min(results)
    if min_val == results[0]:
        return [min_val, "Park 1"]
    elif min_val == results[1]:
        return [min_val, "Park 2"]
    elif min_val == results[2]:
        return [min_val, "Park 3"]
    elif min_val == results[3]:
        return [min_val, "Park 4"]
    else:
        return [0,"0"]

df['distance_to_parks'] = df.apply(cal_dist1, axis=1)
df[['distance_to_nearest_park', 'nearest_park',]] = pd.DataFrame(df.distance_to_parks.tolist(), index=df.index)
df.drop('distance_to_parks', axis=1,inplace=True)
df.sample(2)
```


# EDA:

## Price Distribution:
We would like the distribution of price, where the distribution is located and how much is the spread.
```{python}
fig = px.histogram(
    df, 
    x='price', 
    template= 'ggplot2',
    title="Price Histogram",
    labels={
        'count': 'Count', #this doesnt work 
        'price': 'Price ($)'}
    )

fig.update_layout(yaxis_title='Count')
fig.show()  
```

\

We note the price distribution have high variation. The distribution is postively skewed, so the mean price may not be a godd representative of the data. 

# Minimum Nights Distributions:
Let's have a look at the distribution of minimum nights of the listings, showing only up to minimum of 180 nights:
```{python}
fig = px.histogram(
    df[df['minimum_nights'] <=181],
    x='minimum_nights',
    labels={
        'count': 'Count',
        'minimum_nights': 'Minimum Nights '
    },
    title='Minimum Nights Histogram',
    template='ggplot2'
    )
fig.update_layout(yaxis_title='Count')
fig.show()
```

\ 
We can see that, most listing rent from 1 ~ 3 nights, then there are spike in listings with integer multiple of 30 days (i.e. 30, 60, 90 days).


# Listing and Parks Locations on Map:
We would like how the listing is distributed on the map, and where the location of the parks in the area.
```{python}
#center the map around montreal
map_osm = folium.Map(location=[df['latitude'].mean() , df['longitude'].mean()] , zoom_start=10)
temp = df
#adding a title
title_html = '''
             <h3 align="center" style="font-size:20px"><b>Concetration of listing and park locations.</b></h3>
             '''
map_osm.get_root().html.add_child(folium.Element(title_html))
#make the listing location as list
location = list(zip(df.latitude, df.longitude))
folium.plugins.HeatMap(location, min_opacity = 0.09, max_opacity=0.5).add_to(map_osm)

#make the park's location as list
obs = list(zip(df3['Latitude'], df3['Longitude']))
park_list = ['Park 1', "Park 2", "Park 3", 'Park 4']

for i , el in enumerate(obs):
    #folium.CircleMarker(el[0:2], radius=15, color='#000000', fill_color='#000000').add_to(map_osm)
    folium.Marker(el, popup=park_list[i], icon=folium.Icon(icon='tree', prefix= 'fa', color='green')).add_to(map_osm)
#FastMarkerCluster(data=obs).add_to(map_osm)

map_osm
```

\ 

The heatmap represent the concentration of listing, where the green icon represent the parks. We can see the park number 4 is overlapping with high concetration of listing. After seeing what it's look on the map, we would like to quantify the results, and make a recommendations. 


# Nearest Parks and Distance to Nearest Parks:

Lets have a look at the new columns descriptive statistic:
```{python}
print(df.distance_to_nearest_park.describe())
fig = px.histogram(
    df,
    x='distance_to_nearest_park',
    labels={'distance_to_nearest_park': 'Distance to nearest park (km)'},
    title='Distance to nearest park distribution.',
    template='ggplot2'
)
fig.update_layout(yaxis_title='Count')
fig.show()
```

\ 
Note most llisting have nearest park 
Note that the distance between most listing and their respective nearest park are around 5.5 km. Also, the range of distance to nearest park is from  20 m up to 41.7 km. 

Lets see what park is close to most listing 
```{python}
fig = px.bar(
    df,
    x = df.nearest_park.unique(),
    y = df.nearest_park.value_counts(),
    title= "Number of Listing Near the Parks",
    labels={
        'y': 'Number of Listing',
        'x': 'Park'
    },
    template='ggplot2'
)
fig.show()
```

\

We see that park number 4 has the highest nearest listings, then park 3 with significt drop from 10000 to 2000 listings, then park 2, and finally park number 1. 

\ 

Afterward, we would like to look the nearest neighbourhood to any park. We will use the mean of distance to nearest park for each neighbourhood, and show the six lowest distance mean.

```{python}
df_neig_dis = df.groupby(['neighbourhood']).agg({'distance_to_nearest_park': ['mean','std','count']})
df_neig_dis_sorted = df_neig_dis.distance_to_nearest_park.sort_values('mean').head(6)
df_neig_dis_sorted = pd.DataFrame(df_neig_dis_sorted)
df_neig_dis_sorted.index

fig = px.bar(
    df_neig_dis_sorted,
    x = df_neig_dis_sorted.index,
    y = 'mean',
    color = 'count',
    title= "Mean distance of nearest park organized by neighbourhood.",
    labels={
        'mean': 'Mean distance of nearest park (km)',
        'neighbourhood': 'Neighourhoods'
    }
)
fig.show()
```

\ 
The color in the above grpah represent the number of listing in the neighbourhood. As shown, Le Plateau-Mont-Royal has the lowest mean of all neighbourhood.


# Conclusion

In conclusion, we use two datasets acquired from both the Inside Airbnb, and canada goverment website. We were able to analyzed the data available. Since the investor want the listing close to parks, we would recommend investing on listing from the following neighbourhoods: 

1) Le Plateau-Mont-Royal
2) Ville-Marie
3) Mercier-Hochelaga-Maisonneuve

